{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n",
    "    Natural language processing(NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topics to learn**\n",
    "\n",
    "1. [Tokenization](#Tokenization) (Pre-trained or Unsupervised)\n",
    "    - [Word Tokenize](#NLTK-word-tokenizer-nltk.word_tokenize())\n",
    "    - [Sentence Tokenize](#NLTK-sentence-tokenizer-nltk.sent_tokenize())\n",
    "    - [Token Span](#NLTK-token-span-WhitespaceTokenizer)\n",
    "    - [Text Processsing using PlaintextCorpusReader](#Simple-Text-Processing-NLTK)\n",
    "        - [Methods](#Methods-in-PlaintextCorpusReader)\n",
    "    - [Using NLTK Text module](#Using-the-Text-method-to-analyze-the-text-nltk.text.Text)\n",
    "        - [Methods](#Methods)\n",
    "2. [Stop Words](#Stop-Words)\n",
    "3. [POS Tagging](#POS-Tagging)\n",
    "4. [Lematization](#Lematization)\n",
    "5. [Stemming](#Stemming)\n",
    "    - [Porter](#Porter-Stemmer)\n",
    "    - [Lancaster](#Lancaster-Stemmer)\n",
    "    - [Snowball / English](#Snowball-Stemmer)\n",
    "6. [Chunking](#Chunking)\n",
    "    - [Steps](#Steps-to-implement-Chunking)\n",
    "7. [Chinking](#Chinking)\n",
    "8. [Named Entity Recognition (NER)](#NER)\n",
    "9. [Word Cloud](#Word-Cloud)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "     The conversion of a string of words to a list of words is called tokenization\n",
    "\n",
    "Resources:\n",
    "1. <a href=\"http://www.tulane.edu/~howard/NLP/nlp.html#tokenization-again\"> Howard </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK word tokenizer ```nltk.word_tokenize() ```\n",
    "\n",
    "    Return a tokenized copy of text, using NLTK’s recommended word tokenizer (currently an improved \n",
    "    TreebankWordTokenizer along with PunktSentenceTokenizer for the specified language).\n",
    "\n",
    "Parameters\n",
    "- text (str) – text to split into words\n",
    "- language (str) – the model name in the Punkt corpus\n",
    "- preserve_line (bool) – An option to keep the preserve the sentence and not sentence tokenize it.\n",
    "\n",
    "Points\n",
    "- It splits standard contractions, e.g. “don’t” -> “do”, “n’t” and “they’ll” -> “they”, “‘ll.”\n",
    "- It treats most punctuation characters as separate tokens.\n",
    "- It splits off commas and single quotes, when followed by whitespace.\n",
    "- It separates periods that appear at the end of line\n",
    "\n",
    "#### NLTK sentence tokenizer ```nltk.sent_tokenize()``` \n",
    "\n",
    "    Return a sentence-tokenized copy of text, using NLTK’s recommended sentence tokenizer (currently \n",
    "    PunktSentenceTokenizer for the specified language).\n",
    "\n",
    "Parameters\n",
    "- text – text to split into sentences\n",
    "- language – the model name in the Punkt corpus\n",
    "\n",
    "#### NLTK token-span ```WhitespaceTokenizer```\n",
    "\n",
    "    NLTK tokenizers can produce token-spans, represented as tuples of integers having the same semantics as string \n",
    "    slices, to support efficient comparison of tokenizers. (These methods are implemented as generators.)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "S = '''This above all: to thine own self be true, \n",
    "    And it must follow, as the night the day, \n",
    "    Thou canst not then be false to any man.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenizer\n",
    "\n",
    "nltk.word_tokenize(S) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenizer\n",
    "\n",
    "nltk.sent_tokenize(S) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.WhitespaceTokenizer().span_tokenize(S) # Used as Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nltk.WhitespaceTokenizer().span_tokenize(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Text Processing NLTK\n",
    "\n",
    "    One of the reasons for using NLTK is that it relieves us of much of the effort of making a raw text \n",
    "    amenable to computational analysis. It does so by including a module of corpus readers, which pre-process \n",
    "    files for certain tasks or formats. Most of them are specialized for particular corpora, so we will start \n",
    "    with the basic one, called the PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "Reader = PlaintextCorpusReader('./data/', 'review.txt', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Words = Reader.words()\n",
    "print(len(Words))\n",
    "Words[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods in PlaintextCorpusReader\n",
    "- **Reader.raw()** # Returns the string from which the file was read\n",
    "- **Reader.sents()** # Tokenizes the string to a list of lists of of strings, each of which is a sentence,\n",
    "- **Reader.fileids()** # Returns the file that the reader is reading.\n",
    "- **Reader.abspath('review.txt')** # Returns a ```FileSystemPathPointer``` to that file\n",
    "- **Reader.root** # Returns a ```FileSystemPathPointer``` to the current working directory \n",
    "- **Reader.encoding('review.txt')** # Returns the encoding of the file being read.\n",
    "- **Reader.readme()** # Returns the Readme for the file which is not there in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the ```Text``` method to analyze the text ```nltk.text.Text```\n",
    "\n",
    "    The text methods of Text provide a shortcut to text analysis.\n",
    "\n",
    "#### Methods\n",
    "\n",
    " \n",
    "- collocations(num=20, window_size=2)\n",
    "        \n",
    "        A collocation is a group of words that occur together frequently in a text.\n",
    "        Print collocations derived from the text, ignoring stopwords.\n",
    "\n",
    "- collocation_list(num=20, window_size=2)\n",
    "        \n",
    "        Return collocations derived from the text, ignoring stopwords.\n",
    "\n",
    "- common_contexts(words, num=20)\n",
    " \n",
    "        Find contexts where the specified words appear; list most frequent common contexts first.\n",
    "\n",
    "- concordance(self, word, width=79, lines=25)\n",
    "    \n",
    "        It is often helpful to know the context of a word. The concordance view shows a certain number of \n",
    "        characters before and after every occurrence of a given word:\n",
    "    \n",
    "- concordance_list(self, word, width=79, lines=25)\n",
    "    \n",
    "        Generate a concordance for \"word\" with the specified context window.\n",
    "        Word matching is not case-sensitive.\n",
    "\n",
    "- similar(self, word, num=20)\n",
    "    \n",
    "        Distributional similarity: find other words which appear in the\n",
    "        same contexts as the specified word; list most similar words first.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textLoader(doc, loc = '', encoding='utf-8'):\n",
    "    from nltk.corpus import PlaintextCorpusReader\n",
    "    from nltk.text import Text\n",
    "    return Text(PlaintextCorpusReader(loc, doc, encoding=encoding).words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = textLoader('review.txt', './data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known bug with\n",
    "# review.collocation() \n",
    "\n",
    "print('; '.join(review.collocation_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.common_contexts(['jackie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.concordance('jackie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.similar('sahara')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "    \n",
    "    Words that are very frequent but not particularly informative are called stop words in computational linguistics. There \n",
    "    is no definitive list for English, but most start with a list drawn up by Martin Porter and organized by \n",
    "    grammatical form    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "with open(\"./data/review.txt\", 'r') as f:\n",
    "    raw_text = f.read()\n",
    "f.close()\n",
    "\n",
    "tokenized = word_tokenize(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.probability import FreqDist\n",
    "\n",
    "freq_dist = FreqDist(t for t in tokenized)\n",
    "freq_dist.plot(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The above graph contain words like 'the' or some punctuations like '.' which are irrelevant and not usefull.** \n",
    "\n",
    "Let's remove them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "eng_stop = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(eng_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words and punctuations from the list of words\n",
    "cleaned = [word for word in tokenized if word not in eng_stop and word.isalnum()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_dist = FreqDist(t for t in cleaned)\n",
    "cleaned_dist.plot(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging\n",
    "\n",
    "    The process of classifying words into their parts of speech and labeling them accordingly is known as part-of-speech \n",
    "    tagging, POS-tagging, or simply tagging.\n",
    "\n",
    "List of **Penn Treebank** POS tags: <a href=https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html>Tags Table</a>\n",
    "\n",
    "<table>\n",
    "    <thead valign=\"bottom\">\n",
    "    <tr class=\"row-odd\">\n",
    "        <th class=\"head\">Description</th>\n",
    "        <th class=\"head\">Tag</th>\n",
    "    </tr>\n",
    "    </thead>\n",
    "    <tbody valign=\"top\">\n",
    "    <tr class=\"row-even\">\n",
    "        <td>predeterminer</td>\n",
    "        <td>PDT</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-odd\">\n",
    "        <td>determiner</td>\n",
    "        <td>DT</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-even\">\n",
    "        <td>determiner, WH</td>\n",
    "        <td>WDT</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-odd\">\n",
    "        <td>determiner, possessive</td>\n",
    "        <td>PRP</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-even\">\n",
    "        <td>cardinal number</td>\n",
    "        <td>CD</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-odd\">\n",
    "        <td>adjectives</td>\n",
    "        <td>JJ, JJR, JJS</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-even\">\n",
    "        <td>nouns, common</td>\n",
    "        <td>NN, NNS</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-odd\">\n",
    "        <td>nouns, proper</td>\n",
    "        <td>NNP, NNPS</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-even\">\n",
    "        <td>pronoun, personal</td>\n",
    "        <td>PRP</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-odd\">\n",
    "        <td>pronoun, WH</td>\n",
    "        <td>WP</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-even\">\n",
    "        <td>verbs, conjugated</td>\n",
    "        <td>VBZ, VBP, VBD</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-odd\">\n",
    "        <td>verb, modal auxiliary</td>\n",
    "        <td>MD</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-even\">\n",
    "        <td>verb, base</td>\n",
    "        <td>VB</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-odd\">\n",
    "        <td>verb, gerund or present participle</td>\n",
    "        <td>VBG</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-even\">\n",
    "        <td><em>to</em>, infinitival</td>\n",
    "        <td>TO</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-odd\">\n",
    "        <td>adverbs</td>\n",
    "        <td>RB, RBR, RBS</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-even\">\n",
    "        <td>preposition (+ some sub. conj.)</td>\n",
    "        <td>IN</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-odd\">\n",
    "        <td>preposition as particle</td>\n",
    "        <td>RP</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-even\">\n",
    "        <td>conjunction, coordinating</td>\n",
    "        <td>CC</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-odd\">\n",
    "        <td>conjunction, subordinating</td>\n",
    "        <td>IN, WRB</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-even\">\n",
    "        <td><em>there</em>, existential</td>\n",
    "        <td>EX</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-odd\">\n",
    "        <td>interjection</td>\n",
    "        <td>UH</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-even\">\n",
    "        <td>foreign word</td>\n",
    "        <td>FW</td>\n",
    "    </tr>\n",
    "    <tr class=\"row-odd\">\n",
    "        <td>punctuation (. = .;?*)</td>\n",
    "        <td>. , :</td>\n",
    "    </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Use this command to see the details of any tags you want information on\n",
    "\n",
    "```python\n",
    "    nltk.help.upenn_tagset('VBG')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "nltk_POS = pos_tag(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltkTags = [pair[1] for pair in nltk_POS]\n",
    "nltkTags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lematization\n",
    "\n",
    "    You are probably aware of the fact that a word can have different forms. A noun like cat can occur as cats, cat’s, or \n",
    "    cats’ (plural, possessive and plural possessive), a verb like to love can also be used as loves, loved, or loving \n",
    "    (third person singular, past tense/past participle or present participle/gerund). An adjective like tall can be used \n",
    "    in comparatives as taller and superlatives as tallest. These are all examples of inflection, in which a base form or \n",
    "    lemma is inflected, here by suffixes. The process of removing inflectional morphology is known as lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "lematized = [wnl.lemmatize(word) for word in cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in zip(cleaned, nltkTags, lematized):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "    \n",
    "    Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of \n",
    "    words known as a lemma. For example: words such as “Likes”, ”liked”, ”likely” and ”liking” will be reduced to “like” \n",
    "    after stemming.\n",
    "    \n",
    "NLTK has at least four modules for stemming.\n",
    "   - Porter Stemmer\n",
    "   - Lancaster Stemmer\n",
    "   - Snowball Stemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Porter Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "PStemmer = PorterStemmer()\n",
    "pstem = [PStemmer.stem(word) for word in cleaned]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "LStemmer = LancasterStemmer()\n",
    "lstem = [LStemmer.stem(word) for word in cleaned]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Snowball Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import EnglishStemmer\n",
    "EStemmer = EnglishStemmer()\n",
    "estem = [EStemmer.stem(word) for word in cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in zip(cleaned, nltkTags, lematized, pstem, lstem, estem):\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunking\n",
    "     \n",
    "   **Def 1.**\n",
    "    \n",
    "    Chunking is a term referring to the process of taking individual pieces of information (chunks) and grouping them \n",
    "    into larger units. By grouping each piece into a large whole, which sometimes correspond to syntactic phrases.\n",
    "    \n",
    "   **Def 2.**\n",
    "    \n",
    "    Chunking is a process of extracting phrases from unstructured text, which means analyzing a sentence to identify the\n",
    "    constituents(Noun Groups, Verbs, verb groups, etc.) However, it does not specify their internal structure, nor their \n",
    "    role in the main sentence.\n",
    "    It works on top of POS tagging. It uses POS-tags as input and provides chunks as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Steps to implement Chunking\n",
    "\n",
    "The first step is devise a label for the chunk. This one is easy:\n",
    "```python\n",
    "    chunkLabel = 'NP'\n",
    "```\n",
    "\n",
    "This module implements a regular-expression-like language for creating tag patterns items are surrounded by angled brackets, <>.\n",
    "\n",
    "Let's check one example, ```<DT> <JJ> <NN>```, but the latter two have alternative forms. These can be incorporated regex-ly, as ```<DT> <JJ.*> <NN.*>```, which can be assigned to a pattern:\n",
    "\n",
    "```python\n",
    "    chunkPattern = '<DT> <JJ.*> <NN.*>'\n",
    "```\n",
    "You now call a method to roll the pattern up into a rule, which includes a string for storing a description, which I will just fill in with the label assigned above:\n",
    "\n",
    "```python\n",
    "    Chunker = ChunkRule(chunkPattern, chunkLabel)\n",
    "```\n",
    "    \n",
    "And you initialize a parser that uses this rule:\n",
    "\n",
    "```python\n",
    "    chunkParser = RegexpChunkParser([Chunker], chunk_label=chunkLabel)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Think about this**\n",
    "\n",
    "    Chunking only makes sense when applied to sentences, so you need the text tokenized into sentences and not words. \n",
    "    But NLTK’s sentence tokenizer leaves each sentence untouched, formatted as a string, so each sentence has to be \n",
    "    tokenized into words. Then the words need to be tagged with their part of speech. Then the tagged words can be chunked \n",
    "    into noun phrases. Then the user should be notified of the result, but the tricky part is that a sentence can have no \n",
    "    NP chunks, one such chunk, or more than one. So a sentence itself has to be scanned for the proper chunks. Let us call \n",
    "    the temporary chunks subtrees. Fortunately, they will be labeled as NP, so only those need be reported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk.regexp import ChunkRule, RegexpChunkParser\n",
    "from nltk import sent_tokenize\n",
    "sentences = sent_tokenize(raw_text)\n",
    "\n",
    "chunkLabel = 'NP'\n",
    "chunkPattern = '<DT> <JJ.*> <NN.*>'\n",
    "Chunker = ChunkRule(chunkPattern, chunkLabel)\n",
    "chunkParser = RegexpChunkParser([Chunker], chunk_label=chunkLabel)\n",
    "\n",
    "for s in sentences:\n",
    "    tokenizedS = word_tokenize(s)\n",
    "    taggedS = pos_tag(tokenizedS)\n",
    "    chunkedS = chunkParser.parse(taggedS)\n",
    "    for subtree in chunkedS.subtrees():\n",
    "        if subtree.label() == chunkLabel:\n",
    "            print(subtree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chinking\n",
    "\n",
    "    Sometimes it is easier to define what we want to exclude from a chunk. We can define a chink to be a sequence of \n",
    "    tokens that is not included in a chunk. \n",
    "    \n",
    "    Chinking is the process of removing a sequence of tokens from a chunk. If the matching sequence of tokens spans an \n",
    "    entire chunk, then the whole chunk is removed; if the sequence of tokens appears in the middle of the chunk, these \n",
    "    tokens are removed, leaving two chunks where there was only one before. If the sequence is at the periphery of the \n",
    "    chunk, these tokens are removed, and a smaller chunk remains.\n",
    "\n",
    "    Example: The little yellow dog barked at the cat\n",
    "\n",
    "<table>\n",
    "    <thead valign=\"bottom\">\n",
    "    <tr>\n",
    "        <th class=\"head\">Method</th>\n",
    "        <th class=\"head\">Entire chunk</th>\n",
    "        <th class=\"head\">Middle of a chunk</th>\n",
    "        <th class=\"head\">End of a chunk</th>\n",
    "    </tr>\n",
    "    </thead>\n",
    "    <tbody valign=\"top\">\n",
    "    <tr>\n",
    "        <td><em>Input</em></td>\n",
    "        <td>[a/DT little/JJ\n",
    "            dog/NN]\n",
    "        </td>\n",
    "        <td>[a/DT little/JJ\n",
    "            dog/NN]\n",
    "        </td>\n",
    "        <td>[a/DT little/JJ\n",
    "            dog/NN]\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><em>Operation</em></td>\n",
    "        <td>Chink \"DT JJ NN\"</td>\n",
    "        <td>Chink \"JJ\"</td>\n",
    "        <td>Chink \"NN\"</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><em>Pattern</em></td>\n",
    "        <td>}DT JJ NN{</td>\n",
    "        <td>}JJ{</td>\n",
    "        <td>}NN{</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><em>Output</em></td>\n",
    "        <td>a/DT little/JJ\n",
    "            dog/NN\n",
    "        </td>\n",
    "        <td>[a/DT] little/JJ\n",
    "            [dog/NN]\n",
    "        </td>\n",
    "        <td>[a/DT little/JJ]\n",
    "            dog/NN\n",
    "        </td>\n",
    "    </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "grammar = r\"\"\"\n",
    "  NP:\n",
    "    {<.*>+}          # Chunk everything\n",
    "    }<VBD|IN>+{      # Chink sequences of VBD and IN\n",
    "  \"\"\"\n",
    "sentence = [(\"the\", \"DT\"), (\"little\", \"JJ\"), (\"yellow\", \"JJ\"),\n",
    "       (\"dog\", \"NN\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),  (\"the\", \"DT\"), (\"cat\", \"NN\")]\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "cp.parse(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
