{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n",
    "    Natural language processing(NLP) is a subfield of linguistics, computer science, information engineering, and artificial intelligence concerned with the interactions between computers and human (natural) languages, in particular how to program computers to process and analyze large amounts of natural language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Topics to learn**\n",
    "\n",
    "1. [Tokenization](#Tokenization) (Pre-trained or Unsupervised)\n",
    "    - [Word Tokenize](#NLTK-word-tokenizer-nltk.word_tokenize())\n",
    "    - [Sentence Tokenize](#NLTK-sentence-tokenizer-nltk.sent_tokenize())\n",
    "    - [Token Span](#NLTK-token-span-WhitespaceTokenizer)\n",
    "    - [Text Processsing using PlaintextCorpusReader](#Simple-Text-Processing-NLTK)\n",
    "        - [Methods](#Methods-in-PlaintextCorpusReader)\n",
    "    - [Using NLTK Text module](#Using-the-Text-method-to-analyze-the-text-nltk.text.Text)\n",
    "        - [Methods](#Methods)\n",
    "2. Stemming and Stop Words\n",
    "3. Lematization and POS tagging\n",
    "4. NER, Chunking and Chinking\n",
    "5. Word Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "     The conversion of a string of words to a list of words is called tokenization\n",
    "\n",
    "Resources:\n",
    "1. <a href=\"http://www.tulane.edu/~howard/NLP/nlp.html#tokenization-again\"> Howard </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK word tokenizer ```nltk.word_tokenize() ```\n",
    "\n",
    "    Return a tokenized copy of text, using NLTK’s recommended word tokenizer (currently an improved \n",
    "    TreebankWordTokenizer along with PunktSentenceTokenizer for the specified language).\n",
    "\n",
    "Parameters\n",
    "- text (str) – text to split into words\n",
    "- language (str) – the model name in the Punkt corpus\n",
    "- preserve_line (bool) – An option to keep the preserve the sentence and not sentence tokenize it.\n",
    "\n",
    "Points\n",
    "- It splits standard contractions, e.g. “don’t” -> “do”, “n’t” and “they’ll” -> “they”, “‘ll.”\n",
    "- It treats most punctuation characters as separate tokens.\n",
    "- It splits off commas and single quotes, when followed by whitespace.\n",
    "- It separates periods that appear at the end of line\n",
    "\n",
    "#### NLTK sentence tokenizer ```nltk.sent_tokenize()``` \n",
    "\n",
    "    Return a sentence-tokenized copy of text, using NLTK’s recommended sentence tokenizer (currently \n",
    "    PunktSentenceTokenizer for the specified language).\n",
    "\n",
    "Parameters\n",
    "- text – text to split into sentences\n",
    "- language – the model name in the Punkt corpus\n",
    "\n",
    "#### NLTK token-span ```WhitespaceTokenizer```\n",
    "\n",
    "    NLTK tokenizers can produce token-spans, represented as tuples of integers having the same semantics as string \n",
    "    slices, to support efficient comparison of tokenizers. (These methods are implemented as generators.)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "S = '''This above all: to thine own self be true, \n",
    "    And it must follow, as the night the day, \n",
    "    Thou canst not then be false to any man.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Tokenizer\n",
    "\n",
    "nltk.word_tokenize(S) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Tokenizer\n",
    "\n",
    "nltk.sent_tokenize(S) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.WhitespaceTokenizer().span_tokenize(S) # Used as Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(nltk.WhitespaceTokenizer().span_tokenize(S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Text Processing NLTK\n",
    "\n",
    "    One of the reasons for using NLTK is that it relieves us of much of the effort of making a raw text \n",
    "    amenable to computational analysis. It does so by including a module of corpus readers, which pre-process \n",
    "    files for certain tasks or formats. Most of them are specialized for particular corpora, so we will start \n",
    "    with the basic one, called the PlaintextCorpusReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "Reader = PlaintextCorpusReader('./data/', 'review.txt', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Words = Reader.words()\n",
    "print(len(Words))\n",
    "Words[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods in PlaintextCorpusReader\n",
    "- **Reader.raw()** # Returns the string from which the file was read\n",
    "- **Reader.sents()** # Tokenizes the string to a list of lists of of strings, each of which is a sentence,\n",
    "- **Reader.fileids()** # Returns the file that the reader is reading.\n",
    "- **Reader.abspath('review.txt')** # Returns a FileSystemPathPointer to that file\n",
    "- **Reader.root** # Returns a FileSystemPathPointer` to the current working directory \n",
    "- **Reader.encoding('review.txt')** # Returns the encoding of the file being read.\n",
    "- **Reader.readme()** # Returns the Readme for the file which is not there in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the ```Text``` method to analyze the text ```nltk.text.Text```\n",
    "\n",
    "    The text methods of Text provide a shortcut to text analysis.\n",
    "\n",
    "#### Methods\n",
    "\n",
    " \n",
    "- collocations(num=20, window_size=2)\n",
    "        \n",
    "        A collocation is a group of words that occur together frequently in a text.\n",
    "        Print collocations derived from the text, ignoring stopwords.\n",
    "\n",
    "- collocation_list(num=20, window_size=2)\n",
    "        \n",
    "        Return collocations derived from the text, ignoring stopwords.\n",
    "\n",
    "- common_contexts(words, num=20)\n",
    " \n",
    "        Find contexts where the specified words appear; list most frequent common contexts first.\n",
    "\n",
    "- concordance(self, word, width=79, lines=25)\n",
    "    \n",
    "        It is often helpful to know the context of a word. The concordance view shows a certain number of \n",
    "        characters before and after every occurrence of a given word:\n",
    "    \n",
    "- concordance_list(self, word, width=79, lines=25)\n",
    "    \n",
    "        Generate a concordance for \"word\" with the specified context window.\n",
    "        Word matching is not case-sensitive.\n",
    "\n",
    "- similar(self, word, num=20)\n",
    "    \n",
    "        Distributional similarity: find other words which appear in the\n",
    "        same contexts as the specified word; list most similar words first.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def textLoader(doc, loc = '', encoding='utf-8'):\n",
    "    from nltk.corpus import PlaintextCorpusReader\n",
    "    from nltk.text import Text\n",
    "    return Text(PlaintextCorpusReader(loc, doc, encoding=encoding).words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = textLoader('review.txt', './data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Known bug with\n",
    "# review.collocation() \n",
    "\n",
    "print('; '.join(review.collocation_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.common_contexts(['jackie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.concordance('jackie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review.similar('sahara')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
